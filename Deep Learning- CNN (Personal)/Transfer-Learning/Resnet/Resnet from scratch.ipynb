{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"amkWDwUqfRej"},"source":["import numpy as np\n","from keras import layers\n","from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n","from keras.models import Model, load_model\n","from keras.initializers import glorot_uniform\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Hc1tK-2kn20L","executionInfo":{"status":"ok","timestamp":1628960703193,"user_tz":-360,"elapsed":411,"user":{"displayName":"DS with Bappy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqatVICIncJQfWG3BDBOh6wj36U4eGZR_c0YCl=s64","userId":"09492527370048361012"}},"outputId":"dc70b097-a835-4c12-b78e-ef98ed94f959"},"source":["ROOT = '/content/drive/MyDrive/DL-CV-NLP/CNN/Flowers Recognition'\n","os.chdir(ROOT)\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/DL-CV-NLP/CNN/Flowers Recognition'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRRLnO2fn9GC","executionInfo":{"status":"ok","timestamp":1628960719124,"user_tz":-360,"elapsed":383,"user":{"displayName":"DS with Bappy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqatVICIncJQfWG3BDBOh6wj36U4eGZR_c0YCl=s64","userId":"09492527370048361012"}},"outputId":"e899a65f-ca18-4573-f5bc-de54919b38ab"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'Attempt 3.ipynb'\n","'Build a CNN architecture using Flowers Dataset.ipynb'\n"," Copy_of_Model.ipynb\n"," flowers\n"," Model.ipynb\n"," model.weights.best.hdf5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1cYWkw0HoDaj"},"source":["from keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H90pSgvGoM6h"},"source":["train_datagen = ImageDataGenerator(rescale= 1./255,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   horizontal_flip= True)\n","\n","test_datagen = ImageDataGenerator(rescale= 1./255)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYE2ZNNwoM9E","executionInfo":{"status":"ok","timestamp":1628960885502,"user_tz":-360,"elapsed":509,"user":{"displayName":"DS with Bappy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqatVICIncJQfWG3BDBOh6wj36U4eGZR_c0YCl=s64","userId":"09492527370048361012"}},"outputId":"eb67a617-2175-4032-9ae0-d12fd791b3e0"},"source":["training_set = train_datagen.flow_from_directory(directory='/content/drive/MyDrive/DL-CV-NLP/CNN/Flowers Recognition/flowers/train',\n","                                                 target_size=(224,224),\n","                                                 batch_size=32,\n","                                                 class_mode='categorical')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 3523 images belonging to 5 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Na2cc5l6o2Yb","executionInfo":{"status":"ok","timestamp":1628960987553,"user_tz":-360,"elapsed":2197,"user":{"displayName":"DS with Bappy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqatVICIncJQfWG3BDBOh6wj36U4eGZR_c0YCl=s64","userId":"09492527370048361012"}},"outputId":"74d2b23d-2ed7-4fe6-ff7d-76f9c038953d"},"source":["test_set = test_datagen.flow_from_directory(directory='/content/drive/MyDrive/DL-CV-NLP/CNN/Flowers Recognition/flowers/validation',\n","                                                 target_size=(224,224),\n","                                                 batch_size=32,\n","                                                 class_mode='categorical')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 800 images belonging to 5 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mfwZKLVamkMe"},"source":["## Reduce block"]},{"cell_type":"code","metadata":{"id":"hgWAcRecmdsc"},"source":["def bottleneck_residual_block(X, f, filters, stage, block, reduce=False, s=2):\n","    \"\"\"    \n","    Arguments:\n","    X -- input tensor of shape (m, height, width, channels)\n","    f -- integer, specifying the shape of the middle CONV's window for the main path\n","    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n","    stage -- integer, used to name the layers, depending on their position in the network\n","    block -- string/character, used to name the layers, depending on their position in the network\n","    \n","    reduce -- boolean, True = identifies the reduction layer at the beginning of each learning stage\n","    s -- integer, strides\n","    \n","    Returns:\n","    X -- output of the identity block, tensor of shape (H, W, C)\n","    \"\"\"\n","    \n","    # defining name basis\n","    conv_name_base = 'res' + str(stage) + block + '_branch'\n","    bn_name_base = 'bn' + str(stage) + block + '_branch'\n","    \n","    # Retrieve Filters\n","    F1, F2, F3 = filters\n","    \n","    # Save the input value. You'll need this later to add back to the main path. \n","    X_shortcut = X\n","    \n","    if reduce:\n","        # if we are to reduce the spatial size, apply a 1x1 CONV layer to the shortcut path\n","        # to do that, we need both CONV layers to have similar strides \n","        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","        X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","        X = Activation('relu')(X)\n","        \n","        X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n","                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n","        X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n","    else: \n","        # First component of main path\n","        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n","        X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n","        X = Activation('relu')(X)\n","    \n","    # Second component of main path\n","    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n","    X = Activation('relu')(X)\n","\n","    # Third component of main path\n","    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n","    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n","\n","    # Final step: Add shortcut value to main path, and pass it through a RELU activation \n","    X = Add()([X, X_shortcut])\n","    X = Activation('relu')(X)\n","    \n","    return X\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYCGIe8SpYkE"},"source":["def ResNet50(input_shape, classes):\n","    \"\"\"\n","    Arguments:\n","    input_shape -- tuple shape of the images of the dataset\n","    classes -- integer, number of classes\n","\n","    Returns:\n","    model -- a Model() instance in Keras\n","    \"\"\"\n","\n","    # Define the input as a tensor with shape input_shape\n","    X_input = Input(input_shape)\n","\n","    # Stage 1\n","    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X_input)\n","    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n","\n","    # Stage 2\n","    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='a', reduce=True, s=1)\n","    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='b')\n","    X = bottleneck_residual_block(X, 3, [64, 64, 256], stage=2, block='c')\n","\n","    # Stage 3 \n","    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='a', reduce=True, s=2)\n","    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='b')\n","    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='c')\n","    X = bottleneck_residual_block(X, 3, [128, 128, 512], stage=3, block='d')\n","\n","    # Stage 4 \n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='a', reduce=True, s=2)\n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='b')\n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='c')\n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='d')\n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='e')\n","    X = bottleneck_residual_block(X, 3, [256, 256, 1024], stage=4, block='f')\n","\n","    # Stage 5 \n","    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='a', reduce=True, s=2)\n","    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='b')\n","    X = bottleneck_residual_block(X, 3, [512, 512, 2048], stage=5, block='c')\n","\n","    # AVGPOOL \n","    X = AveragePooling2D((1,1), name=\"avg_pool\")(X)\n","\n","    # output layer\n","    X = Flatten()(X)\n","    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n","    \n","    # Create the model\n","    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVwGY9QBpgCb"},"source":["model = ResNet50(input_shape = (224,224, 3), classes = 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYJRjWDvp1O_"},"source":["model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnoXHP6aqF0Y","executionInfo":{"status":"ok","timestamp":1628962143221,"user_tz":-360,"elapsed":834760,"user":{"displayName":"DS with Bappy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgqatVICIncJQfWG3BDBOh6wj36U4eGZR_c0YCl=s64","userId":"09492527370048361012"}},"outputId":"119bc9e1-a3fb-4623-ab9a-22137c25280d"},"source":["hist = model.fit(training_set,\n","                         steps_per_epoch = 10,\n","                         epochs = 5,\n","                         validation_data = test_set,    \n","                         validation_steps = 20)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","10/10 [==============================] - 396s 37s/step - loss: 87.4699 - accuracy: 0.1590 - val_loss: 5390.7065 - val_accuracy: 0.2313\n","Epoch 2/5\n","10/10 [==============================] - 145s 15s/step - loss: 12.9741 - accuracy: 0.2318 - val_loss: 1.7806 - val_accuracy: 0.2109\n","Epoch 3/5\n","10/10 [==============================] - 101s 10s/step - loss: 16.7177 - accuracy: 0.2283 - val_loss: 2.0752 - val_accuracy: 0.2062\n","Epoch 4/5\n","10/10 [==============================] - 81s 8s/step - loss: 22.0307 - accuracy: 0.2126 - val_loss: 1.8783 - val_accuracy: 0.2000\n","Epoch 5/5\n","10/10 [==============================] - 78s 8s/step - loss: 13.5529 - accuracy: 0.2024 - val_loss: 2.8115 - val_accuracy: 0.1969\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cAIB5vh4qPz9"},"source":[],"execution_count":null,"outputs":[]}]}